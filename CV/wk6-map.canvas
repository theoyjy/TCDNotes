{
	"nodes":[
		{"id":"d0cdd42af3b4c479","type":"text","text":"## K-Nearest Neighbor\n\nClassified by a **majority vote** of its neighbors, with the object being assigned to the class most common its k neighbors (k is positive integer, typically small)","x":231,"y":-920,"width":529,"height":180},
		{"id":"44eb0da9edda60af","type":"text","text":"## Neural Network\n* **Perceptron**: take several binary inputs and produces a single binary output.\n* **Weights and biases**: need to learn $w +\\triangle w$  -> the aim if for small change in weight to cause only a small corresponding change in the output from the network.\n![[Pasted image 20241112145446.png|400]]\n- Modern perceptron -- **Neuron**:\n  Not only can output binary results, but also have non-linear activation functions:\n  * ReLU(Rectified Linear Unit)![[Pasted image 20241112151422.png|100]] Popular in hidden layer, quick\n  * **Sigmoid** $g(x) = \\frac {e^x} {1+e^x}$ can **avoid** small change in *weight or bias of any single perceptron* to cause the **output completely flip**. Map values to range between 0 to 1\n  * **Tanh** function: map values to a range between -1 and 1. its **zero-centered output** helps **prevent bias shifts during training** (->used in hidden layers), leading to *potentially faster convergence*\n","x":231,"y":-680,"width":649,"height":802},
		{"id":"3ffff67aae6d6c9e","type":"text","text":"Sigmoid Benefits:\nsmall changes in the input x don’t cause the output to “flip” suddenly but rather adjust **gradually**. This **helps in stabilizing the learning process and keeps the network’s output more predictable and less sensitive** to small variations in weights and biases.","x":1260,"y":-830,"width":498,"height":190},
		{"id":"fd6a63bf4a3ac89c","type":"text","text":"### Learning with gradient descent\nCost Function $C(w, b) = \\frac {1} {2n} \\sum {||y(x)-a||^2}$\n![[Pasted image 20241112213006.png]]\n\n* **Learning rate $\\eta$**\n\nThe gradient of a function C at a point v, gives the direction in which C increases the most rapidly.\n\nTo minimize the function, we want to **decrease** C. Therefore, we move in the **opposite direction** of the gradient, which points toward the steepest descent.\n\n![[Pasted image 20241112213556.png|450]]","x":1260,"y":-560,"width":520,"height":880},
		{"id":"b0b088b9652fee4c","x":1960,"y":-780,"width":694,"height":560,"type":"text","text":"### Learning with stochastic 随机 gradient descent(SGD)\n\nUsing subsets of sample rather than whole training data -> faster the updates\n\n![[Pasted image 20241112214051.png]]"},
		{"id":"ef837a2b7e5a6c2a","x":1957,"y":90,"width":883,"height":830,"type":"text","text":"### Backpropagation Algorithm\nThe primary goal of backpropagation is to **minimize the cost function C**\n![[Pasted image 20241112220521.png|800]]\nBecause C depends on w indirectly (through a and z), the **chain rule** is applied. For a weight w in the last layer, the partial derivative $\\frac{\\partial C}{\\partial w}$$​ can be broken down as:\n$$\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$\nWith assuming the $\\frac{\\partial C}{\\partial a} = 1$ \n![[Pasted image 20241112224049.png|700]]\n\n* After calculating the gradient of C over wi, the finally we can update the wi with learning rate $\\eta$"},
		{"id":"99849a1de281199f","x":2960,"y":203,"width":629,"height":272,"type":"text","text":"- **Add** gate: gradient **distributor** (**pass** the derivative directly)\n- **Max** gate: gradient **router**(In backpropagation, the gradient flows **back only** to the input that was the **maximum**, since the other inputs are treated as 0)\n- **Mul** gate: gradient **switcher**(each input’s **gradient** is “switched” by **the value of the other input**)"}
	],
	"edges":[
		{"id":"24a6967e1d5ecfd7","fromNode":"44eb0da9edda60af","fromSide":"right","toNode":"3ffff67aae6d6c9e","toSide":"left"},
		{"id":"dd61791d49963227","fromNode":"44eb0da9edda60af","fromSide":"right","toNode":"fd6a63bf4a3ac89c","toSide":"left"},
		{"id":"93536f2a67941296","fromNode":"fd6a63bf4a3ac89c","fromSide":"right","toNode":"b0b088b9652fee4c","toSide":"left"},
		{"id":"105e86673d7fd7bb","fromNode":"fd6a63bf4a3ac89c","fromSide":"right","toNode":"ef837a2b7e5a6c2a","toSide":"left"},
		{"id":"df20ac2dc4569e63","fromNode":"99849a1de281199f","fromSide":"left","toNode":"ef837a2b7e5a6c2a","toSide":"right"}
	]
}