# Lec 1
Supervised machine learning
- classification
  Target values are *discrete*
- regression 
  Target values are *continues*

classifier(functions) train by training data to predict.
* when training/input data is a number vector, such as pixels in image, called **feature vector***
1. map from real data to numeric feature vector
2. train function h(x)
   
We need to label data
Data can be unrepresentative, noisy/unreliable, not useful relationship(correlation vs causation)

### example -- sensation of movie reviews
1. delete uninteresting words -- stop words
2. truncate word endings -- stemming-- get rid of the tense
3. map text of reviews to an array, `array["word"] = NumberOfAppearance`
4. Assign weight $\theta_{i}$ to work $i$

---
# Lec 2 - Linear Regression
$x$ input variables
$y$ target values
$\theta_{0} \quad \theta_{1}$ are unknown parameters
* Prediction: $y = h_{\theta}(x) = \theta_{0} + \theta_{1}x$
* Idea is to choose $\theta_{0}$ and $\theta_{1}$ so that $h_{\theta_{0}}(x^{(i)})$ is close to $y^{i}$ for each of our training examples
* Least square case : cost function:
  ![[wk01-20240913121715773.webp|292]]
* Optimization: select the values for θ0 and θ1 that minimize cost function

>[!attention] Gradient Descent
>* start with some $\theta_{0}$ and $\theta_{1}$
>* Repeat: update params to new values which makes $J(\theta_{0}, \theta_{1})$ smaller:
>  Stopping criteria: stop when decreases by less than e.g. 10−2 or after a fixed number of iterations e.g. 200, whichever comes first
>* The curve may have multiple local minimum, converge to that, global minimum may be unachievable

>[!question] Derivatives

- normalization both $x$ and $y$

### Select Step Size
- too small -> long time
- too large -> overshoot the minimum
- adjust step $\alpha$ at each iteration
































