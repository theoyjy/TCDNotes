---
annotation-target: TCD-SCSS-DISSERTATION-2022-023.pdf
---


>%%
>```annotation-json
>{"created":"2024-10-02T20:43:04.227Z","text":"Problem trying to solve","updated":"2024-10-02T20:43:04.227Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":1409,"end":1484},{"type":"TextQuoteSelector","exact":"Turning pages of sheet music is a common source of irritation for musicians","prefix":"ervisor: Dr. Kenneth Dawson-Howe","suffix":", andthis paper aims to address "}]}]}
>```
>%%
>*%%PREFIX%%ervisor: Dr. Kenneth Dawson-Howe%%HIGHLIGHT%% ==Turning pages of sheet music is a common source of irritation for musicians== %%POSTFIX%%, andthis paper aims to address*
>%%LINK%%[[#^4dzbn6rujw4|show annotation]]
>%%COMMENT%%
>Problem trying to solve
>%%TAGS%%
>
^4dzbn6rujw4



>%%
>```annotation-json
>{"created":"2024-10-02T20:44:23.481Z","text":"The method of solving it","updated":"2024-10-02T20:44:23.481Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":1614,"end":1754},{"type":"TextQuoteSelector","exact":"a tablet device and explores the viability ofusing gaze detection as a method of triggering automatic page turns in a sheet musicapplication","prefix":"lication has been developed for ","suffix":". Creating a passive interface w"}]}]}
>```
>%%
>*%%PREFIX%%lication has been developed for%%HIGHLIGHT%% ==a tablet device and explores the viability ofusing gaze detection as a method of triggering automatic page turns in a sheet musicapplication== %%POSTFIX%%. Creating a passive interface w*
>%%LINK%%[[#^867bnufg9a|show annotation]]
>%%COMMENT%%
>The method of solving it
>%%TAGS%%
>
^867bnufg9a


>%%
>```annotation-json
>{"created":"2024-10-02T20:48:00.054Z","text":"examine the performance of 2 gaze detection systems and 3 page-turning  systems","updated":"2024-10-02T20:48:00.054Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":1954,"end":2069},{"type":"TextQuoteSelector","exact":"the effect that various gaze detection solutions andpage-turning systems have on the performance of the application","prefix":"rns. In an effort to understand ","suffix":" two different gazedetection sol"}]}]}
>```
>%%
>*%%PREFIX%%rns. In an effort to understand%%HIGHLIGHT%% ==the effect that various gaze detection solutions andpage-turning systems have on the performance of the application== %%POSTFIX%%two different gazedetection sol*
>%%LINK%%[[#^sy64lsh2pnh|show annotation]]
>%%COMMENT%%
>examine the performance of 2 gaze detection systems and 3 page-turning  systems
>%%TAGS%%
>
^sy64lsh2pnh


>%%
>```annotation-json
>{"created":"2024-10-02T22:00:10.884Z","text":"lighting and positioning is serious challenge","updated":"2024-10-02T22:00:10.884Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":2407,"end":2526},{"type":"TextQuoteSelector","exact":" challenges with performinggaze detection in a general environment with uncontrolled lighting or positioning of theuser","prefix":"than others, and highlighted the","suffix":".Contents1 Introduction 21.1 Bac"}]}]}
>```
>%%
>*%%PREFIX%%than others, and highlighted the%%HIGHLIGHT%% ==challenges with performinggaze detection in a general environment with uncontrolled lighting or positioning of theuser== %%POSTFIX%%.Contents1 Introduction 21.1 Bac*
>%%LINK%%[[#^ie5hwd7d8x|show annotation]]
>%%COMMENT%%
>lighting and positioning is serious challenge
>%%TAGS%%
>
^ie5hwd7d8x


>%%
>```annotation-json
>{"created":"2024-10-02T22:03:47.682Z","text":"page turning is a long-time issue","updated":"2024-10-02T22:03:47.682Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":3723,"end":3854},{"type":"TextQuoteSelector","exact":"in fact this is so common that sheet music publishers willsometimes arrange music so that page turns land at a more convenient time","prefix":"eirinstrument to turn the page, ","suffix":". Thewidespread digitisation of "}]}]}
>```
>%%
>*%%PREFIX%%eirinstrument to turn the page,%%HIGHLIGHT%% ==in fact this is so common that sheet music publishers willsometimes arrange music so that page turns land at a more convenient time== %%POSTFIX%%. Thewidespread digitisation of*
>%%LINK%%[[#^7i7h36vxa6y|show annotation]]
>%%COMMENT%%
>page turning is a long-time issue
>%%TAGS%%
>
^7i7h36vxa6y



>%%
>```annotation-json
>{"created":"2024-10-02T22:06:15.805Z","text":"problems of current digital music sheet applications","updated":"2024-10-02T22:06:15.805Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":799,"end":841},{"type":"TextQuoteSelector","exact":"tablet device could be placed horizontally","prefix":"aller in size thanan A4 page. A ","suffix":" which allows two pages to bedis"}]}]}
>```
>%%
>*%%PREFIX%%aller in size thanan A4 page. A%%HIGHLIGHT%% ==tablet device could be placed horizontally== %%POSTFIX%%which allows two pages to bedis*
>%%LINK%%[[#^dht3zhw7fd4|show annotation]]
>%%COMMENT%%
>problems of current digital music sheet applications
>%%TAGS%%
>
^dht3zhw7fd4


>%%
>```annotation-json
>{"created":"2024-10-02T22:09:27.569Z","updated":"2024-10-02T22:09:27.569Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":5107,"end":5260},{"type":"TextQuoteSelector","exact":"to create an automatic page turner there must be a system whichkeeps track of the musicians place in the sheet music and appropriately triggers pageturns","prefix":"n in howpage turns are handled, ","suffix":". This dissertation will explore"}]}]}
>```
>%%
>*%%PREFIX%%n in howpage turns are handled,%%HIGHLIGHT%% ==to create an automatic page turner there must be a system whichkeeps track of the musicians place in the sheet music and appropriately triggers pageturns== %%POSTFIX%%. This dissertation will explore*
>%%LINK%%[[#^bifj12qw5i|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^bifj12qw5i


>%%
>```annotation-json
>{"created":"2024-10-02T22:10:56.594Z","updated":"2024-10-02T22:10:56.594Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":5887,"end":5898},{"type":"TextQuoteSelector","exact":"distraction","prefix":" of turning pages can also be a ","suffix":" for musicians. Therehave been s"}]}]}
>```
>%%
>*%%PREFIX%%of turning pages can also be a%%HIGHLIGHT%% ==distraction== %%POSTFIX%%for musicians. Therehave been s*
>%%LINK%%[[#^2bjg7vd1geb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^2bjg7vd1geb


>%%
>```annotation-json
>{"created":"2024-10-02T22:11:25.646Z","text":"current solutions of semi-automatic page-turning all must be manually triggered","updated":"2024-10-02T22:11:25.646Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":6096,"end":6114},{"type":"TextQuoteSelector","exact":"manually triggered","prefix":"e in common is that theymust be ","suffix":". One of the key focuses for thi"}]}]}
>```
>%%
>*%%PREFIX%%e in common is that theymust be%%HIGHLIGHT%% ==manually triggered== %%POSTFIX%%. One of the key focuses for thi*
>%%LINK%%[[#^59e0g68q49x|show annotation]]
>%%COMMENT%%
>current solutions of semi-automatic page-turning all must be manually triggered
>%%TAGS%%
>
^59e0g68q49x


>%%
>```annotation-json
>{"created":"2024-10-02T22:16:25.408Z","text":"One Key focus","updated":"2024-10-02T22:16:25.408Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":6116,"end":6299},{"type":"TextQuoteSelector","exact":"One of the key focuses for this project was to design a passiveinterface which involved minimal active interactions from the user, thus minimising thedistraction caused by page turns.","prefix":"theymust be manually triggered. ","suffix":" There are various possibilities"}]}]}
>```
>%%
>*%%PREFIX%%theymust be manually triggered.%%HIGHLIGHT%% ==One of the key focuses for this project was to design a passiveinterface which involved minimal active interactions from the user, thus minimising thedistraction caused by page turns.== %%POSTFIX%%There are various possibilities*
>%%LINK%%[[#^c7vjdoly5dw|show annotation]]
>%%COMMENT%%
>One Key focus
>%%TAGS%%
>
^c7vjdoly5dw


>%%
>```annotation-json
>{"created":"2024-10-02T22:18:03.807Z","text":"two mechanism involved in a auto turning pages system","updated":"2024-10-02T22:18:03.807Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":6386,"end":6478},{"type":"TextQuoteSelector","exact":"mechanism for displaying new music on-screen as wellas the system to trigger such page turns","prefix":"-turning interface; both in the ","suffix":". Gaze detection was chosen as t"}]}]}
>```
>%%
>*%%PREFIX%%-turning interface; both in the%%HIGHLIGHT%% ==mechanism for displaying new music on-screen as wellas the system to trigger such page turns== %%POSTFIX%%. Gaze detection was chosen as t*
>%%LINK%%[[#^bzwap29q3d8|show annotation]]
>%%COMMENT%%
>two mechanism involved in a auto turning pages system
>%%TAGS%%
>
^bzwap29q3d8


>%%
>```annotation-json
>{"created":"2024-10-02T22:37:29.269Z","text":"The journal uses a high-end research-grade device(SMI RED500 eye-gaze tracker) to evaluate the Kalman filter model with real eye-gaze data, rather than a widely accessible commercial product for general consumer use","updated":"2024-10-02T22:37:29.269Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":6702,"end":6949},{"type":"TextQuoteSelector","exact":"Although there has already been research focused on using gaze detection toturn the pages of sheet music (Tabone et al., 2020), the use of gaze detection performedwith commercial hardware in a general setting has not been studied for this use case","prefix":" page-turning from themusician. ","suffix":". Assuch, this dissertation aims"}]}]}
>```
>%%
>*%%PREFIX%%page-turning from themusician.%%HIGHLIGHT%% ==Although there has already been research focused on using gaze detection toturn the pages of sheet music (Tabone et al., 2020), the use of gaze detection performedwith commercial hardware in a general setting has not been studied for this use case== %%POSTFIX%%. Assuch, this dissertation aims*
>%%LINK%%[[#^s1xf37263zf|show annotation]]
>%%COMMENT%%
>The journal uses a high-end research-grade device(SMI RED500 eye-gaze tracker) to evaluate the Kalman filter model with real eye-gaze data, rather than a widely accessible commercial product for general consumer use
>%%TAGS%%
>
^s1xf37263zf


>%%
>```annotation-json
>{"created":"2024-10-02T22:48:40.347Z","updated":"2024-10-02T22:48:40.347Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":7317,"end":7394},{"type":"TextQuoteSelector","exact":" evaluating gaze detection as a trigger mechanism forsheet music page-turning","prefix":"wThis dissertation will focus on","suffix":", as well as comparing between d"}]}]}
>```
>%%
>*%%PREFIX%%wThis dissertation will focus on%%HIGHLIGHT%% ==evaluating gaze detection as a trigger mechanism forsheet music page-turning== %%POSTFIX%%, as well as comparing between d*
>%%LINK%%[[#^wthai01npui|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^wthai01npui


>%%
>```annotation-json
>{"created":"2024-10-02T22:48:48.137Z","updated":"2024-10-02T22:48:48.137Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":7407,"end":7455},{"type":"TextQuoteSelector","exact":"comparing between different page-turning systems","prefix":" music page-turning, as well as ","suffix":".An application will be develope"}]}]}
>```
>%%
>*%%PREFIX%%music page-turning, as well as%%HIGHLIGHT%% ==comparing between different page-turning systems== %%POSTFIX%%.An application will be develope*
>%%LINK%%[[#^3thdsy87zxr|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^3thdsy87zxr


>%%
>```annotation-json
>{"created":"2024-10-02T22:51:35.076Z","updated":"2024-10-02T22:51:35.076Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":9593,"end":9685},{"type":"TextQuoteSelector","exact":"This animation isquite important as it gives the user an indication that the page is turning","prefix":"brings the next page into view. ","suffix":", helping userskeep track of whe"}]}]}
>```
>%%
>*%%PREFIX%%brings the next page into view.%%HIGHLIGHT%% ==This animation isquite important as it gives the user an indication that the page is turning== %%POSTFIX%%, helping userskeep track of whe*
>%%LINK%%[[#^06djid35ohk3|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^06djid35ohk3


>%%
>```annotation-json
>{"created":"2024-10-02T22:53:49.352Z","updated":"2024-10-02T22:53:49.352Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":9926,"end":9977},{"type":"TextQuoteSelector","exact":" using real-time audio of a performance as a trigge","prefix":"v, 2007).Research in the area of","suffix":"r forpage-turning focused on ali"}]}]}
>```
>%%
>*%%PREFIX%%v, 2007).Research in the area of%%HIGHLIGHT%% ==using real-time audio of a performance as a trigge== %%POSTFIX%%r forpage-turning focused on ali*
>%%LINK%%[[#^hnk2ak67gf|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^hnk2ak67gf


>%%
>```annotation-json
>{"created":"2024-10-02T23:29:33.391Z","updated":"2024-10-02T23:29:33.391Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":10006,"end":10213},{"type":"TextQuoteSelector","exact":"aligning the audio of a live performance with that of asynthesised audio file of the piece the musician is playing (Arzt et al., 2008). By aligning thetwo audio files, an estimate of the musicianʼs progress ","prefix":"gger forpage-turning focused on ","suffix":"through the piece could becalcul"}]}]}
>```
>%%
>*%%PREFIX%%gger forpage-turning focused on%%HIGHLIGHT%% ==aligning the audio of a live performance with that of asynthesised audio file of the piece the musician is playing (Arzt et al., 2008). By aligning thetwo audio files, an estimate of the musicianʼs progress== %%POSTFIX%%through the piece could becalcul*
>%%LINK%%[[#^bhg7l5ej5v6|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^bhg7l5ej5v6


>%%
>```annotation-json
>{"created":"2024-10-02T23:29:55.265Z","updated":"2024-10-02T23:29:55.265Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":10324,"end":10448},{"type":"TextQuoteSelector","exact":"gaze-detection as a trigger for page turns, using aninfrared-based eye tracking device: the SMI RED500 (Tabone et al., 2020)","prefix":"rns.A recent study investigated ","suffix":". Kalman filteringwas used to sm"}]}]}
>```
>%%
>*%%PREFIX%%rns.A recent study investigated%%HIGHLIGHT%% ==gaze-detection as a trigger for page turns, using aninfrared-based eye tracking device: the SMI RED500 (Tabone et al., 2020)== %%POSTFIX%%. Kalman filteringwas used to sm*
>%%LINK%%[[#^x1q9f0dcv2j|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^x1q9f0dcv2j


>%%
>```annotation-json
>{"created":"2024-10-02T23:32:03.974Z","updated":"2024-10-02T23:32:03.974Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":10978,"end":11075},{"type":"TextQuoteSelector","exact":"the goal of gaze detection or eye tracking is to understand where a subject islooking in 3D space","prefix":" et al., 1950). Broadlyspeaking ","suffix":". There are a number of differen"}]}]}
>```
>%%
>*%%PREFIX%%et al., 1950). Broadlyspeaking%%HIGHLIGHT%% ==the goal of gaze detection or eye tracking is to understand where a subject islooking in 3D space== %%POSTFIX%%. There are a number of differen*
>%%LINK%%[[#^ayhd7w9fp7q|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ayhd7w9fp7q


>%%
>```annotation-json
>{"created":"2024-10-02T23:40:51.604Z","updated":"2024-10-02T23:40:51.604Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":11159,"end":11168},{"type":"TextQuoteSelector","exact":"intrusive","prefix":" problem, some ofwhich are more ","suffix":" than others. One of the less co"}]}]}
>```
>%%
>*%%PREFIX%%problem, some ofwhich are more%%HIGHLIGHT%% ==intrusive== %%POSTFIX%%than others. One of the less co*
>%%LINK%%[[#^t7wphitx8rk|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^t7wphitx8rk


>%%
>```annotation-json
>{"created":"2024-10-02T23:45:42.678Z","text":"笨重","updated":"2024-10-02T23:45:42.678Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":12607,"end":12617},{"type":"TextQuoteSelector","exact":"cumbersome","prefix":"ystems were originally largeand ","suffix":", but have been miniaturised and"}]}]}
>```
>%%
>*%%PREFIX%%ystems were originally largeand%%HIGHLIGHT%% ==cumbersome== %%POSTFIX%%, but have been miniaturised and*
>%%LINK%%[[#^lku5uvd4f8a|show annotation]]
>%%COMMENT%%
>笨重
>%%TAGS%%
>
^lku5uvd4f8a


>%%
>```annotation-json
>{"created":"2024-10-02T23:45:52.266Z","text":"微型化","updated":"2024-10-02T23:45:52.266Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":12633,"end":12645},{"type":"TextQuoteSelector","exact":"miniaturised","prefix":"geand cumbersome, but have been ","suffix":" and stylised with time. Some ex"}]}]}
>```
>%%
>*%%PREFIX%%geand cumbersome, but have been%%HIGHLIGHT%% ==miniaturised== %%POSTFIX%%and stylised with time. Some ex*
>%%LINK%%[[#^rpbwxvo0uf|show annotation]]
>%%COMMENT%%
>微型化
>%%TAGS%%
>
^rpbwxvo0uf


>%%
>```annotation-json
>{"created":"2024-10-02T23:46:10.109Z","text":"环境光","updated":"2024-10-02T23:46:10.109Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":12782,"end":12795},{"type":"TextQuoteSelector","exact":"ambient light","prefix":"nted systems use infraredlight: ","suffix":" is also used, as is the case wi"}]}]}
>```
>%%
>*%%PREFIX%%nted systems use infraredlight:%%HIGHLIGHT%% ==ambient light== %%POSTFIX%%is also used, as is the case wi*
>%%LINK%%[[#^o9xgtgow2k8|show annotation]]
>%%COMMENT%%
>环境光
>%%TAGS%%
>
^o9xgtgow2k8


>%%
>```annotation-json
>{"created":"2024-10-02T23:47:18.371Z","updated":"2024-10-02T23:47:18.371Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":13073,"end":13305},{"type":"TextQuoteSelector","exact":"The remaining eye tracking systems are less intrusive but still rely on videoanalysis. These remaining systems can be broken down into three main categories:● Infrared-based systems● Geometric-based systems● Appearance-based systems","prefix":"to determine the gaze direction.","suffix":"There are non-head-mounted eye t"}]}]}
>```
>%%
>*%%PREFIX%%to determine the gaze direction.%%HIGHLIGHT%% ==The remaining eye tracking systems are less intrusive but still rely on videoanalysis. These remaining systems can be broken down into three main categories:● Infrared-based systems● Geometric-based systems● Appearance-based systems== %%POSTFIX%%There are non-head-mounted eye t*
>%%LINK%%[[#^5lucpfn0f5k|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5lucpfn0f5k


>%%
>```annotation-json
>{"created":"2024-10-02T23:48:13.018Z","updated":"2024-10-02T23:48:13.018Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":13528,"end":13579},{"type":"TextQuoteSelector","exact":"Geometric-based systems build a 3D model of the eye","prefix":"stems can be seen in Figure 2.1.","suffix":" and use geometric reasoningto e"}]}]}
>```
>%%
>*%%PREFIX%%stems can be seen in Figure 2.1.%%HIGHLIGHT%% ==Geometric-based systems build a 3D model of the eye== %%POSTFIX%%and use geometric reasoningto e*
>%%LINK%%[[#^jmxrp60pbpn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^jmxrp60pbpn


>%%
>```annotation-json
>{"created":"2024-10-02T23:48:28.874Z","updated":"2024-10-02T23:48:28.874Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":13699,"end":13813},{"type":"TextQuoteSelector","exact":" identify a subjectʼs face within an image and then features such as the eye contour andpupil location are detecte","prefix":"013). Facial detection is usedto","suffix":"d. These features are used to bu"}]}]}
>```
>%%
>*%%PREFIX%%013). Facial detection is usedto%%HIGHLIGHT%% ==identify a subjectʼs face within an image and then features such as the eye contour andpupil location are detecte== %%POSTFIX%%d. These features are used to bu*
>%%LINK%%[[#^8wh4396grtx|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^8wh4396grtx


>%%
>```annotation-json
>{"created":"2024-10-02T23:49:01.700Z","updated":"2024-10-02T23:49:01.700Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":13885,"end":14021},{"type":"TextQuoteSelector","exact":"subjectʼs head pose must also be identified, as the combination of the head poseand pupil directions are used to calculate a gaze vector","prefix":" model of a subject ʻseyes. The ","suffix":". These techniques use depthsens"}]}]}
>```
>%%
>*%%PREFIX%%model of a subject ʻseyes. The%%HIGHLIGHT%% ==subjectʼs head pose must also be identified, as the combination of the head poseand pupil directions are used to calculate a gaze vector== %%POSTFIX%%. These techniques use depthsens*
>%%LINK%%[[#^jesoaslwq3|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^jesoaslwq3


>%%
>```annotation-json
>{"created":"2024-10-02T23:51:31.333Z","updated":"2024-10-02T23:51:31.333Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":13443,"end":13453},{"type":"TextQuoteSelector","exact":"reflection","prefix":"based on the same principles of ","suffix":" from the eye. Anexample of one "}]}]}
>```
>%%
>*%%PREFIX%%based on the same principles of%%HIGHLIGHT%% ==reflection== %%POSTFIX%%from the eye. Anexample of one*
>%%LINK%%[[#^3yppqpk8p6v|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^3yppqpk8p6v


>%%
>```annotation-json
>{"created":"2024-10-02T23:51:37.815Z","updated":"2024-10-02T23:51:37.815Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":14546,"end":14589},{"type":"TextQuoteSelector","exact":"Appearance-based systems use ambient light ","prefix":"cation of the subjectʼs pupils.8","suffix":"and standard video of a subject."}]}]}
>```
>%%
>*%%PREFIX%%cation of the subjectʼs pupils.8%%HIGHLIGHT%% ==Appearance-based systems use ambient light== %%POSTFIX%%and standard video of a subject.*
>%%LINK%%[[#^c3hwbu2ueju|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^c3hwbu2ueju


>%%
>```annotation-json
>{"created":"2024-10-03T00:07:49.703Z","text":"视点","updated":"2024-10-03T00:07:49.703Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":14998,"end":15014},{"type":"TextQuoteSelector","exact":"points of regard","prefix":"n eye within animage with known ","suffix":". This allows for the creation o"}]}]}
>```
>%%
>*%%PREFIX%%n eye within animage with known%%HIGHLIGHT%% ==points of regard== %%POSTFIX%%. This allows for the creation o*
>%%LINK%%[[#^z5wjm6aqu7|show annotation]]
>%%COMMENT%%
>视点
>%%TAGS%%
>
^z5wjm6aqu7


>%%
>```annotation-json
>{"created":"2024-10-03T12:39:43.014Z","text":"crop, unlike patch(a square small piece), crop refer to a meaningful section/feature out of an image like eye, iris","updated":"2024-10-03T12:39:43.014Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":15730,"end":15736},{"type":"TextQuoteSelector","exact":"crops ","prefix":" a deep-learning approach, where","suffix":"of a subject's eyes were used as"}]}]}
>```
>%%
>*%%PREFIX%%a deep-learning approach, where%%HIGHLIGHT%% ==crops== %%POSTFIX%%of a subject's eyes were used as*
>%%LINK%%[[#^n6qdf3qek0q|show annotation]]
>%%COMMENT%%
>crop, unlike patch(a square small piece), crop refer to a meaningful section/feature out of an image like eye, iris
>%%TAGS%%
>
^n6qdf3qek0q


>%%
>```annotation-json
>{"created":"2024-10-03T12:43:30.723Z","text":"calibration 校准","updated":"2024-10-03T12:43:30.723Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":16354,"end":16405},{"type":"TextQuoteSelector","exact":"iTracker doesnʼt require person-specificcalibration","prefix":"tʼs face in the original image. ","suffix":", although it was shown in the 2"}]}]}
>```
>%%
>*%%PREFIX%%tʼs face in the original image.%%HIGHLIGHT%% ==iTracker doesnʼt require person-specificcalibration== %%POSTFIX%%, although it was shown in the 2*
>%%LINK%%[[#^n1ae5klmp7|show annotation]]
>%%COMMENT%%
>calibration 校准
>%%TAGS%%
>
^n1ae5klmp7


>%%
>```annotation-json
>{"created":"2024-10-03T12:44:21.794Z","updated":"2024-10-03T12:44:21.794Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":14621,"end":14757},{"type":"TextQuoteSelector","exact":"Facial-detection techniques are used to isolate a face crop from an image of a subject,then features are extracted from this face image.","prefix":"and standard video of a subject.","suffix":" Examples of these features are "}]}]}
>```
>%%
>*%%PREFIX%%and standard video of a subject.%%HIGHLIGHT%% ==Facial-detection techniques are used to isolate a face crop from an image of a subject,then features are extracted from this face image.== %%POSTFIX%%Examples of these features are*
>%%LINK%%[[#^aikbu6dhiac|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^aikbu6dhiac


>%%
>```annotation-json
>{"created":"2024-10-03T12:44:52.175Z","updated":"2024-10-03T12:44:52.175Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":16740,"end":16794},{"type":"TextQuoteSelector","exact":"they require segmentation of faces and eyes fromimages","prefix":"timation have in common is that ","suffix":". There are several approaches t"}]}]}
>```
>%%
>*%%PREFIX%%timation have in common is that%%HIGHLIGHT%% ==they require segmentation of faces and eyes fromimages== %%POSTFIX%%. There are several approaches t*
>%%LINK%%[[#^tqsfndz43n|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^tqsfndz43n


>%%
>```annotation-json
>{"created":"2024-10-03T12:46:00.391Z","text":"IOS playform, itracker and seeso to detect gaze, output of them as trigger of 3 page turning animations.\nAdditional calibration function to both calculate the metrics on the gaze systems and compensate for errors by the gaze detection systems","updated":"2024-10-03T12:46:00.391Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":17345,"end":17380},{"type":"TextQuoteSelector","exact":"EyeGaze app architecture and design","prefix":"velopment of the EyeGaze app3.1 ","suffix":"There were a number of important"}]}]}
>```
>%%
>*%%PREFIX%%velopment of the EyeGaze app3.1%%HIGHLIGHT%% ==EyeGaze app architecture and design== %%POSTFIX%%There were a number of important*
>%%LINK%%[[#^votiv2fem7n|show annotation]]
>%%COMMENT%%
>IOS playform, itracker and seeso to detect gaze, output of them as trigger of 3 page turning animations.
>Additional calibration function to both calculate the metrics on the gaze systems and compensate for errors by the gaze detection systems
>%%TAGS%%
>
^votiv2fem7n


>%%
>```annotation-json
>{"created":"2024-10-03T12:55:37.895Z","updated":"2024-10-03T12:55:37.895Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":19322,"end":19482},{"type":"TextQuoteSelector","exact":"The calibration process allows for gaze estimations to be comparedwith known ground truth values, since the locations of the calibration dots arepre-determined.","prefix":"n allows forseveral advantages. ","suffix":" With the assumption that the us"}]}]}
>```
>%%
>*%%PREFIX%%n allows forseveral advantages.%%HIGHLIGHT%% ==The calibration process allows for gaze estimations to be comparedwith known ground truth values, since the locations of the calibration dots arepre-determined.== %%POSTFIX%%With the assumption that the us*
>%%LINK%%[[#^gwkl0wn2zmj|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^gwkl0wn2zmj


>%%
>```annotation-json
>{"created":"2024-10-03T12:59:28.342Z","updated":"2024-10-03T12:59:28.342Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":19633,"end":19674},{"type":"TextQuoteSelector","exact":"metrics are calculated on the performance","prefix":"ared to the actual locations and","suffix":" of the gaze detection system. C"}]}]}
>```
>%%
>*%%PREFIX%%ared to the actual locations and%%HIGHLIGHT%% ==metrics are calculated on the performance== %%POSTFIX%%of the gaze detection system. C*
>%%LINK%%[[#^jtrsk2ser1k|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^jtrsk2ser1k


>%%
>```annotation-json
>{"created":"2024-10-03T12:59:38.824Z","updated":"2024-10-03T12:59:38.824Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":19908,"end":19978},{"type":"TextQuoteSelector","exact":"calibration results can be used to fine-tune the page-turningtechnique","prefix":" calibrationprocess is that the ","suffix":"s. To give an example, if the ga"}]}]}
>```
>%%
>*%%PREFIX%%calibrationprocess is that the%%HIGHLIGHT%% ==calibration results can be used to fine-tune the page-turningtechnique== %%POSTFIX%%s. To give an example, if the ga*
>%%LINK%%[[#^3gf7981fn9b|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^3gf7981fn9b


>%%
>```annotation-json
>{"created":"2024-10-03T13:04:08.800Z","updated":"2024-10-03T13:04:08.800Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":20705,"end":20796},{"type":"TextQuoteSelector","exact":"SeeSooperates directly on a camera feed, whereas iTracker has specific inputs which must be","prefix":"redictions (Visual Camp, 2021). ","suffix":"12prepared before any gaze predi"}]}]}
>```
>%%
>*%%PREFIX%%redictions (Visual Camp, 2021).%%HIGHLIGHT%% ==SeeSooperates directly on a camera feed, whereas iTracker has specific inputs which must be== %%POSTFIX%%12prepared before any gaze predi*
>%%LINK%%[[#^3z0b36b02t7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^3z0b36b02t7


>%%
>```annotation-json
>{"created":"2024-10-03T13:04:16.416Z","updated":"2024-10-03T13:04:16.416Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":20798,"end":20849},{"type":"TextQuoteSelector","exact":"prepared before any gaze predictions are performed.","prefix":" specific inputs which must be12","suffix":" To transform the front-facing c"}]}]}
>```
>%%
>*%%PREFIX%%specific inputs which must be12%%HIGHLIGHT%% ==prepared before any gaze predictions are performed.== %%POSTFIX%%To transform the front-facing c*
>%%LINK%%[[#^k6b0rhckjth|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^k6b0rhckjth


>%%
>```annotation-json
>{"created":"2024-10-03T13:04:38.225Z","updated":"2024-10-03T13:04:38.225Z","document":{"title":"Dissertation","link":[{"href":"urn:x-pdf:cd4b85925a25cef96a53b538f0b9a013"},{"href":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf"}],"documentFingerprint":"cd4b85925a25cef96a53b538f0b9a013"},"uri":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","target":[{"source":"vault:/Research & Innovation/group/TCD-SCSS-DISSERTATION-2022-023.pdf","selector":[{"type":"TextPositionSelector","start":20918,"end":20978},{"type":"TextQuoteSelector","exact":"iTracker, a face and eye segmentation module wasimplemented.","prefix":"frames into suitable inputs for ","suffix":" Although the SeeSo SDK provides"}]}]}
>```
>%%
>*%%PREFIX%%frames into suitable inputs for%%HIGHLIGHT%% ==iTracker, a face and eye segmentation module wasimplemented.== %%POSTFIX%%Although the SeeSo SDK provides*
>%%LINK%%[[#^t9t71dcqr48|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^t9t71dcqr48
